# MERA Industrial

<p align="center">
Â  <picture>
Â  Â  <source media="(prefers-color-scheme: dark)" srcset="docs/mera-industrial-logo.svg">
Â  Â  <source media="(prefers-color-scheme: light)" srcset="docs/mera-industrial-logo-black.svg">
Â  Â  <img alt="MERA Industrial" src="docs/mera-industrial-logo.svg" style="max-width: 100%;">
Â  </picture>
</p>

<p align="center">
Â  Â  <a href="https://opensource.org/licenses/MIT">
Â  Â  <img alt="License" src="https://img.shields.io/badge/License-MIT-yellow.svg">
Â  Â  </a>
Â  Â  <a href="https://github.com/MERA-Evaluation/MERA_Industrial/tree/main">
Â  Â  <img alt="Release" src="https://img.shields.io/badge/release-v1.0.0-blue">
Â  Â  </a>

</p>

<h2 align="center">
Â  Â  <p> MERA Induscrial: A Unified Framework for Evaluating Industrial tasks.
</p>
</h2>

## ğŸš€ About

**MERA Industrial** brings together a domain-specific collection of evaluation tasks under one roof. Built on top of the [Language Model Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness) (v0.4.9), it enables researchers and practitioners to:

- **Compare models** on identical tasks and metrics
- **Reproduce results** with fixed prompts and few-shot settings
- **Submit** standardized ZIP archives for leaderboard integration


## ğŸ” Datasets Overview

| Set Â  Â  Â  Â  | Task Name Â  Â            Â  Â  Â | Metrics Â  Â  Â  Â  Â  Â  Â  Â | Size  | Prompts | Skills Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â |
| ----------- | ---------------------------- | ---------------------- | ----- | ------- | ------------------------------------------------------------- |
| **Private** | **ruTXTMedQFundamental** Â  Â  | ExactMatch, F1 Â  Â  Â  Â  | 4590 Â | 10 Â  Â  Â | Anatomy, Biochemistry, Bioorganic Chemistry, Biophysics, Clinical Laboratory Diagnostics, Faculty Surgery, General Chemistry, General Surgery, Histology, Hygiene, Microbiology, Normal Physiology, Parasitology, Pathological Anatomy, Pathological physiology, Pharmacology, Propaedeutics in Internal Medicine |
| **Private** | **ruTXTAgroBench**           | ExactMatch, F1 Â   Â  Â  Â | 2642  Â | 10 Â  Â  Â | Botany, Forage Production and Grassland Management, Land Reclamation, General Genetics, General Agriculture, Fundamentals of Plant Breeding, Plant Production, Seed Production and Seed Science, Agricultural Systems in Various Agricultural Landscapes, Crop Cultivation Technologies |
| **Private** | **ruTXTAquaBench**           | ExactMatch, F1 Â   Â   Â  | 992  Â | 10 Â  Â  Â | Industrial aquaculture; Ichthyopathology: veterinary medicine, prevention and optimization of fish farming technologies; Feeding fish and other aquatic organisms; Mariculture, Breeding crayfish and shrimp, Artificial pearl cultivation. |


## ğŸ›  Getting Started <a name="evaluation"></a>

### Clone the repository with submodule

First, you need to clone the MERA_CODE repository and load the submodule:

```bash
### Go to the folder where the repository will be cloned ###
mkdir mera_industrial
cd mera_industrial

### Clone & install core libs ###
git clone --recurse-submodules https://github.com/MERA-Evaluation/MERA_Industrial.git
cd MERA_Industrial
```

### Installing dependencies

**Remote Scoring**: quick setup for cloud-based scoring â€” install only core dependencies, run the evaluation, and submit the resulting ZIP archive to our website to get the score. 

Install lm-eval library and optional packages for evaluations:

```bash
### Install lm-eval ###
cd lm-evaluation-harness
pip install -e .

### Install additional libs for models evaluation [Optional] ###
# vLLM engine
pip install -e ".[vllm]"
# API scoring
pip install -e ".[api]"

### Go to MERA_Industrial folder ###
cd ../
```

### Running evaluations

We have prepared the script that launches evaluations via `lm-eval` library and packs the evaluation logs into zip archive:

```bash
### Run evaluation and pack logs ###
bash scripts/run_evaluation.sh \
 --model vllm \
Â --model_args "pretrained=Qwen/Qwen2.5-0.5B-Instruct,tensor_parallel_size=1" \
 --output_path "./results/Qwen2.5-0.5B-Instruct"
```

More details on `run_evaluation.sh` usage may be obtained by:
```bash
bash scripts/run_evaluation.sh --help
```

<details>
<summary>
How it works inside...
</summary>

```bash
### run lm-eval
lm_eval \
    --model vllm \  # use vLLM engile
    --model_args pretrained=Qwen/Qwen2.5-0.5B-Instruct,dtype=bfloat16 \  # model init details
    --log_samples \  # save eval logs (model generations)
    --device cuda \  # inference on cuda
    --batch_size=1 \  # use batch_size=1
    --verbosity ERROR \  # only essential prints
    --output_path="./results/Qwen2.5-0.5B-Instruct" \  # where to save the logs
    --include_path industrial_tasks/ \  # include out custom tasks
    --trust_remote_code \  # may be needed for some models
    --apply_chat_template \  # use chat template of the model
    --fewshot_as_multiturn \  # along with apply_chat_template, matters only for num_fewshot > 0
    --tasks agro_bench,aqua_bench,med_bench  # eval tasks
  
### pack logs into zip archive
python scripts/log_to_submission.py \
    --outputs_dir ./results/Qwen2.5-0.5B-Instruct \
    --dst_dir ./results/Qwen2.5-0.5B-Instruct \
    --model_args pretrained=Qwen/Qwen2.5-0.5B-Instruct,dtype=bfloat16

### there would appear results/Qwen2.5-0.5B-Instruct_submission.zip archive ready for submission
```

</details>

## ğŸ“ Repository Structure

```text
MERA_CODE/
â”œâ”€â”€ industrial_tasks/ Â  Â  Â  Â  Â   # Code for each task
â”œâ”€â”€ datasets/ Â  Â  Â  Â  Â  Â  Â  Â  Â  Â # Task descriptions, metadata, readme
â”œâ”€â”€ docs/ Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â # Additional documentation and design notes
 â”œâ”€â”€ dataset_formatting.md Â  Â  Â  # Dataset formatting requirements
 â”œâ”€â”€ model_scoring.md Â  Â  Â  Â  Â  Â # How to use lm-eval to evaluate the LMs
 â”œâ”€â”€ task_codebase.md Â  Â  Â  Â  Â  Â # How to add a new task to the codebase
â”œâ”€â”€ lm-evaluation-harness/ Â  Â  Â Â # Submodule (codebase)
â””â”€â”€ scripts/ Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Helpers: add tasks, run evaluations, and scoring
```


## ğŸ’ª How to Join the Leaderboard

Follow these steps to see your model on the Leaderboard:

1. **Run Remote Scoring** Â 
 Evaluate the benchmark in the **Remote Scoring** regime (see [ğŸ›  Getting Started](#evaluation) above). Pay attention that for **private** tasks we do not provide golden answers, so no local scoring is provided.
 > Youâ€™ll end up with a logs folder **and** a ready-to-submit zip archive like `Qwen2.5-0.5B-Instruct_submission.zip`.

2. **Submit on the website** Â 
 Head over to [Create Submission](https://mera.a-ai.ru/ru/industrial/submits/create), upload the archive, and move on to the form.

3. **Fill in Model Details** Â 
 Provide accurate information about the model and evaluation. These details are crucial for reproducibilityâ€”if something is missing, administrators may ping you (or your Submission might be rejected).

4. **Wait for Scoring** â³ Â 
 Scoring usually wraps up in **~10-15 minutes**.

1. **Publish your result** Â 
 Once scoring finishes, click **"Submit for moderation"**. After approval, your model goes **Public** and appears on the [Leaderboard](https://mera.a-ai.ru/ru/industrial/leaderboard). Â 

Good luck, and happy benchmarking! ğŸ‰
Â  Â 
## ğŸ“ License

Distributed under the MIT License. See LICENSE for details.
